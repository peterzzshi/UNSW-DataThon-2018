{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============ Datathon INSTRUCTIONS and INFORMATION ============\n",
    "\n",
    "Challenge: SkyHigh Books want to know in 2018 what readers will rate their books that they publish. You are given data from 2017, where you know what readers read and what rating each user provided on average to all books they read.\n",
    "\n",
    "Your job is 4 fold:\n",
    "\n",
    "Predict what readers will rate on average in 2018 on all books they will read in 2017 + 2018. Use the wishlist of books readers want to read in 2018 and what they have already read in 2017 as a starting point. SkyHigh Books wants to know this information, as they want to market directly to readers who either have higher potential average scores on all books, or lower scores.\n",
    "\n",
    "SkyHigh Books wants to know why some books in 2017 or 2018 (predict which books) have good or bad overall ratings. Is it certain words in the book? The genre of the book? The price in 2017 / 2018? Where it was sold? Why do some books on average have a higher rating than other books?\n",
    "\n",
    "SkyHigh Books wants to maintain a good online presence on book review sites (hence we want to predict user averages), and so they want you to tell them how they can lift overall global reader satisifcation (not just each individual user, but overall).\n",
    "\n",
    "Provide confidence bounds on your predictions. How much % are you sure your predictions are true or correct? Is the results plausible? Does the data seem plausible? (Ie do the word count follow some Power Law Distribution?)\n",
    "\n",
    "**Datasets**\n",
    "\n",
    "You are provided with 6 datasets:\n",
    "\n",
    "1. **Books Information**\n",
    "2. **Genres Mapping**\n",
    "3. **User Data**\n",
    "4. **Words in Book**\n",
    "5. **Words Mapping**\n",
    "6. **Example Submissions**\n",
    "\n",
    "------\n",
    "\n",
    "1. **Books Information** Actual info on books. Book ID, Barcode, difficulty (average *perceieved* reading difficulty of book --> 1 = easy, 5 = hard).\n",
    "2. **Genres Mapping** Maps Genre IDs to real Genre types (eg Science)\n",
    "3. **User Data** User's data on average ratings and what they read in 2017 + wishlist for 2018.\n",
    "4. **Words in Book** What are the words in the actual book.\n",
    "5. **Words Mapping** Actual words mapped to word ID\n",
    "6. **Example Submissions** Shows an example of what you need to provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn import datasets, metrics\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder, OneHotEncoder, RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## loading data\n",
    "Books_Information = pd.read_csv('Books information.csv')\n",
    "User_Data = pd.read_csv('User Data.csv').replace(\"Not specified\", np.nan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>User Difficulty Choice</th>\n",
       "      <th>User Read Books (2017)</th>\n",
       "      <th>User Read Books (2018)</th>\n",
       "      <th>Average Rating (2017)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID790145788</td>\n",
       "      <td>1</td>\n",
       "      <td>6254</td>\n",
       "      <td>7180</td>\n",
       "      <td>3.115447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID646234447</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5854, 5676, 7265, 6051</td>\n",
       "      <td>5729, 5279, 6242, 5519, 5251</td>\n",
       "      <td>3.157876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID86918707</td>\n",
       "      <td>5</td>\n",
       "      <td>5815</td>\n",
       "      <td>6372, 6697</td>\n",
       "      <td>1.512284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID404262345</td>\n",
       "      <td>2</td>\n",
       "      <td>6969, 7002, 5761</td>\n",
       "      <td>5023, 6412, 6124</td>\n",
       "      <td>3.641681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID267634255</td>\n",
       "      <td>4</td>\n",
       "      <td>7908, 7585, 7655, 5013, 5352, 7753, 5793</td>\n",
       "      <td>7915, 7884, 7404, 6022, 6589, 6543, 5450, 6848</td>\n",
       "      <td>2.506743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       User ID User Difficulty Choice  \\\n",
       "0  ID790145788                      1   \n",
       "1  ID646234447                    NaN   \n",
       "2   ID86918707                      5   \n",
       "3  ID404262345                      2   \n",
       "4  ID267634255                      4   \n",
       "\n",
       "                     User Read Books (2017)  \\\n",
       "0                                      6254   \n",
       "1                    5854, 5676, 7265, 6051   \n",
       "2                                      5815   \n",
       "3                          6969, 7002, 5761   \n",
       "4  7908, 7585, 7655, 5013, 5352, 7753, 5793   \n",
       "\n",
       "                           User Read Books (2018)  Average Rating (2017)  \n",
       "0                                            7180               3.115447  \n",
       "1                    5729, 5279, 6242, 5519, 5251               3.157876  \n",
       "2                                      6372, 6697               1.512284  \n",
       "3                                5023, 6412, 6124               3.641681  \n",
       "4  7915, 7884, 7404, 6022, 6589, 6543, 5450, 6848               2.506743  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "User_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book ID</th>\n",
       "      <th>Book Genre</th>\n",
       "      <th>Barcode</th>\n",
       "      <th>Difficulty (Reader suggested)</th>\n",
       "      <th>Number Of Words</th>\n",
       "      <th>Price (2017)</th>\n",
       "      <th>Price (2018)</th>\n",
       "      <th>Most Sold At</th>\n",
       "      <th>Number Sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5000</td>\n",
       "      <td>24</td>\n",
       "      <td>#24-349283</td>\n",
       "      <td>2</td>\n",
       "      <td>937</td>\n",
       "      <td>32.546216</td>\n",
       "      <td>34.030877</td>\n",
       "      <td>Daniel's Bookshop</td>\n",
       "      <td>2789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5001</td>\n",
       "      <td>30</td>\n",
       "      <td>#30-441083</td>\n",
       "      <td>1</td>\n",
       "      <td>4258</td>\n",
       "      <td>12.955205</td>\n",
       "      <td>13.122562</td>\n",
       "      <td>Daniel's Bookshop</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5002</td>\n",
       "      <td>26</td>\n",
       "      <td>#26-417947</td>\n",
       "      <td>2</td>\n",
       "      <td>4063</td>\n",
       "      <td>7.793854</td>\n",
       "      <td>8.311120</td>\n",
       "      <td>DHC-Online</td>\n",
       "      <td>4282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5003</td>\n",
       "      <td>19</td>\n",
       "      <td>#19-346900</td>\n",
       "      <td>5</td>\n",
       "      <td>2868</td>\n",
       "      <td>9.871150</td>\n",
       "      <td>10.156169</td>\n",
       "      <td>Level 5 Daniel Shop</td>\n",
       "      <td>3294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5004</td>\n",
       "      <td>21</td>\n",
       "      <td>#21-349180</td>\n",
       "      <td>2</td>\n",
       "      <td>4622</td>\n",
       "      <td>44.802459</td>\n",
       "      <td>46.369520</td>\n",
       "      <td>Daniel's Bookshop</td>\n",
       "      <td>739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Book ID  Book Genre     Barcode  Difficulty (Reader suggested)  \\\n",
       "0     5000          24  #24-349283                              2   \n",
       "1     5001          30  #30-441083                              1   \n",
       "2     5002          26  #26-417947                              2   \n",
       "3     5003          19  #19-346900                              5   \n",
       "4     5004          21  #21-349180                              2   \n",
       "\n",
       "   Number Of Words  Price (2017)  Price (2018)         Most Sold At  \\\n",
       "0              937     32.546216     34.030877    Daniel's Bookshop   \n",
       "1             4258     12.955205     13.122562    Daniel's Bookshop   \n",
       "2             4063      7.793854      8.311120           DHC-Online   \n",
       "3             2868      9.871150     10.156169  Level 5 Daniel Shop   \n",
       "4             4622     44.802459     46.369520    Daniel's Bookshop   \n",
       "\n",
       "   Number Sold  \n",
       "0         2789  \n",
       "1          411  \n",
       "2         4282  \n",
       "3         3294  \n",
       "4          739  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Books_Information.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Book ID                          0\n",
       "Book Genre                       0\n",
       "Barcode                          0\n",
       "Difficulty (Reader suggested)    0\n",
       "Number Of Words                  0\n",
       "Price (2017)                     0\n",
       "Price (2018)                     0\n",
       "Most Sold At                     0\n",
       "Number Sold                      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to ensure no missing values in the entries\n",
    "Books_Information.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_2017 = User_Data.iloc[:, [0, 1, 2, 4]]\n",
    "arr_2017 = [[columns[0], columns[1], int(book), float(columns[3])] for columns in user_2017.values for book in columns[2].split(', ')]\n",
    "user_2017 = pd.DataFrame(arr_2017, columns=user_2017.columns)\n",
    "\n",
    "user_2018 = User_Data.iloc[:, [0, 1, 3]]\n",
    "arr_2018 = [[columns[0], columns[1], int(book)] for columns in user_2018.values for book in columns[2].split(', ')]\n",
    "user_2018 = pd.DataFrame(arr_2018, columns=user_2018.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_2017 = user_2017.merge(Books_Information, left_on='User Read Books (2017)', right_on='Book ID')\n",
    "df_2018 = user_2018.merge(Books_Information, left_on='User Read Books (2018)', right_on='Book ID')\n",
    "\n",
    "df_2017 = df_2017[[\"User ID\", \"User Read Books (2017)\", \"User Difficulty Choice\", \"Book Genre\", \"Difficulty (Reader suggested)\", \"Number Of Words\", \"Price (2017)\", \"Most Sold At\", \"Number Sold\", \"Average Rating (2017)\"]].rename(columns={'User Read Books (2017)': 'Book', 'Price (2017)': 'Price', 'Average Rating (2017)': \"Rating\"})\n",
    "df_2018 = df_2018[[\"User ID\", \"User Read Books (2018)\", \"User Difficulty Choice\", \"Book Genre\", \"Difficulty (Reader suggested)\", \"Number Of Words\", \"Price (2018)\", \"Most Sold At\", \"Number Sold\"]].rename(columns={'User Read Books (2018)': 'Book', 'Price (2018)': 'Price'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df_2017.iloc[:, 2:8].values\n",
    "y = df_2017.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Taking care of missing data\n",
    "\n",
    "imputer = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "imputer = imputer.fit(X[:, 0].reshape(-1, 1))\n",
    "X[:, 0] = imputer.transform(X[:, 0].reshape(-1, 1)).round().reshape(-1, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "X[:, 5] = labelencoder.fit_transform(X[:, 5])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1, 5])\n",
    "X = onehotencoder.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''# feature scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X = sc_X.fit_transform(X)\n",
    "sc_y = StandardScaler()\n",
    "y = sc_y.fit_transform(y.reshape(-1, 1))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "def CV_RMSE(model):\n",
    "    kf = KFold(n_folds, shuffle=True)\n",
    "    scores = np.sqrt(-cross_val_score(model, X, y, cv=kf.get_n_splits(X), scoring='neg_mean_squared_error'))\n",
    "    print(scores)\n",
    "    print(\"Mean: \")\n",
    "    print(scores.mean())\n",
    "    print(\"Standard Deviation: \")\n",
    "    print(scores.std())\n",
    "#     return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Score: [ 1.60129741  1.63104755  1.57574771  1.6367075   1.58524533  1.6236834\n",
      "  1.63482202  1.65051586  1.653381    1.64618522]\n",
      "1.62386330049 0.0259466992685\n"
     ]
    }
   ],
   "source": [
    "elasticnet = ElasticNet(alpha=0.003, l1_ratio=0.5, random_state=100, selection='cyclic')\n",
    "\n",
    "scores = CV_RMSE(elasticnet)\n",
    "print(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Score: [ 1.65410044  1.68172519  1.65939165  1.68420128  1.62352286  1.66060431\n",
      "  1.67072673  1.71888523  1.70147595  1.68715353]\n",
      "1.67417871814 0.0254038369031\n"
     ]
    }
   ],
   "source": [
    "svr = SVR()\n",
    "\n",
    "scores = CV_RMSE(SVR())\n",
    "print(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Score: [ 1.60249777  1.63301277  1.57764625  1.63692596  1.58526251  1.62175701\n",
      "  1.63468013  1.64969968  1.65170222  1.64760477]\n",
      "1.62407890623 0.0254244474108\n"
     ]
    }
   ],
   "source": [
    "ramdomforest = RandomForestRegressor(n_estimators=100, max_depth=5, min_samples_leaf=7, max_features=\"sqrt\")\n",
    "\n",
    "scores = CV_RMSE(ramdomforest)\n",
    "print(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Score: [ 1.60420217  1.63264358  1.5745859   1.63873419  1.58754405  1.62193697\n",
      "  1.63688704  1.65646649  1.6530388   1.64952707]\n",
      "1.6255566268 0.026738752788\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor(learning_rate=0.01)\n",
    "\n",
    "scores = CV_RMSE(gbr)\n",
    "print(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE Score: [ 1.60455668  1.63326943  1.5745859   1.63878476  1.58758016  1.62193697\n",
      "  1.63688704  1.65651508  1.65361455  1.64948551]\n",
      "1.62572160957 0.026786504121\n"
     ]
    }
   ],
   "source": [
    "gbr = GradientBoostingRegressor()\n",
    "\n",
    "scores = CV_RMSE(gbr)\n",
    "print(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"elasticnet\": elasticnet,\n",
    "    \"svr\": svr,\n",
    "    \"ramdomforest\": ramdomforest,\n",
    "    \"gbr\": gbr\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regressor elasticnet has RMSE score:\n",
      "[ 1.60129741  1.63104755  1.57574771  1.6367075   1.58524533  1.6236834\n",
      "  1.63482202  1.65051586  1.653381    1.64618522]\n",
      "Mean: \n",
      "1.62386330049\n",
      "Standard Deviation: \n",
      "0.0259466992685\n",
      "\n",
      "regressor svr has RMSE score:\n"
     ]
    }
   ],
   "source": [
    "for name, regressor in models.items():\n",
    "    print(\"regressor \" + name + \" has RMSE score:\")\n",
    "    CV_RMSE(regressor)\n",
    "#     print(\"Mean: \" + scores.mean() + \"Standard Deviation: \" + scores.std())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in [100, 300, 500, 700, 900, 1000]:\n",
    "    scores = CV_RMSE(XGBRegressor(learning_rate=0.1, n_estimators=n))\n",
    "    print(scores.mean(), scores.std())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "plt.plot(alpha_ranges, alpha_scores)\n",
    "plt.xlabel('Value of alpha for Ridge')\n",
    "plt.ylabel('Cross-Validated RMSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# LASSO Regression :\n",
    "lasso = Lasso(alpha=0.0005, random_state=1)\n",
    "# Elastic Net Regression\n",
    "ENet = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)\n",
    "# Gradient Boosting Regression\n",
    "GBoost = GradientBoostingRegressor(\n",
    "    n_estimators=3000,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    max_features='sqrt',\n",
    "    min_samples_leaf=15,\n",
    "    min_samples_split=10,\n",
    "    loss='huber',\n",
    "    random_state=5)\n",
    "#  XGboost\n",
    "model_xgb = xgb.XGBRegressor(\n",
    "    colsample_bytree=0.4603,\n",
    "    gamma=0.0468,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    min_child_weight=1.7817,\n",
    "    n_estimators=2200,\n",
    "    reg_alpha=0.4640,\n",
    "    reg_lambda=0.8571,\n",
    "    subsample=0.5213,\n",
    "    silent=1,\n",
    "    random_state=7,\n",
    "    nthread=-1)\n",
    "\n",
    "# Base models scores\n",
    "score = CV_RMSE(lasso)\n",
    "print(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "score = CV_RMSE(ENet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "# score = rmsle_cv(KRR)\n",
    "# print(\n",
    "#     \"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "score = CV_RMSE(GBoost)\n",
    "print(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(),\n",
    "                                                          score.std()))\n",
    "score = CV_RMSE(model_xgb)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))\n",
    "# score = rmsle_cv(model_lgb)\n",
    "# print(\"LGBM score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "\n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "\n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack(\n",
    "            [model.predict(X) for model in self.models_])\n",
    "        return np.mean(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 评价这四个模型的好坏\n",
    "averaged_models = AveragingModels(models=(ENet, GBoost, model_xgb, lasso))\n",
    "score = rmsle_cv(averaged_models)\n",
    "print(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(),\n",
    "                                                              score.std()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "\n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "\n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "\n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "\n",
    "    # Do the predictions of all base models on the test data and use the averaged predictions as\n",
    "    # meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(\n",
    "                axis=1) for base_models in self.base_models_\n",
    "        ])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "stacked_averaged_models = StackingAveragedModels(\n",
    "    base_models=(ENet, GBoost, model_xgb), meta_model=lasso)\n",
    "score = rmsle_cv(stacked_averaged_models)\n",
    "print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(),\n",
    "                                                               score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clone(lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rmsle_cv(5, BaggingRegressor(base_estimator=ridge, n_estimators=param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = [1, 10, 20, 40, 60]\n",
    "test_scores = []\n",
    "for param in params:\n",
    "    clf = BaggingRegressor(base_estimator=ridge, n_estimators=param)\n",
    "    # cv=5表示cross_val_score采用的是k-fold cross validation的方法，重复5次交叉验证\n",
    "    # scoring='precision'、scoring='recall'、scoring='f1', scoring='neg_mean_squared_error' 方差值\n",
    "    test_score = np.sqrt(-cross_val_score(clf, X_train, y_train.ravel(), cv=10, scoring='neg_mean_squared_error'))\n",
    "    test_scores.append(np.mean(test_score))\n",
    "print(test_score.mean())\n",
    "plt.plot(params, test_scores)\n",
    "plt.title('CV Error vs No. estimators')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KFold(5, shuffle=True, random_state=42).get_n_splits(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Validation function\n",
    "n_folds = 5\n",
    "\n",
    "\n",
    "def rmsle_cv(model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42)\n",
    "    rmse = np.sqrt(-cross_val_score(\n",
    "        model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv=kf.get_n_splits(train.values)))\n",
    "    print(\"rmse\", rmse)\n",
    "    return (rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
